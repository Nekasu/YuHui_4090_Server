{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 之前的文档介绍了一些基础的工具, 但并未介绍整个工作的流程\n",
    "- 本篇文章介绍了整个神经网络搭建、训练、保存的工作流程\n",
    "- 如果在未来的什么时候忘记了工作流程, 可以来这里查看\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进行优化过程前, 我们需要准备好数据与网络, 我们将使用之前介绍的[Dataset与DataLoader](./2_Dataset_DataLoader_两个与数据加载相关的类.ipynb)进行数据加载, 并使用[torch.nn](./4_搭建神经网络的工具_torch.nn包.ipynb)搭建神经网络\n",
    "\n",
    "需要的数据有：\n",
    "1. 训练集与测试集-Dataset类的实例\n",
    "2. 训练集与测试集的数据加载器-DataLoader类的实例\n",
    "3. 神经网络模型-神经网络类的实例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备数据集的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    transform=ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    transform=ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(dataset=training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义神经网络的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=28*28, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个神经网络的实例, 如果需要, 可以使用如cuda的硬件进行加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = NeuralNetwork().to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除此之外, 还需要设置一些超参数：\n",
    "1. 学习率\n",
    "2. bath_size\n",
    "3. epoch\n",
    "\n",
    "如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数的设置\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "# epochs=2意味着将在整个数据集上进行2次训练+测试的过程\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、训练与测试过程的创立"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在准备好数据后, 我们需要定义优化过程与测试过程\n",
    "- 这两个过程往往以函数的形式出现\n",
    "- 一般命名为`train_loop`与`test_loop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义train_loop函数\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    # 将模型设置为“训练模式”\n",
    "    # 这个语句不是必须的, 但是加上能提升模型的性能, 并降低过拟合的概率\n",
    "    model.train()\n",
    "\n",
    "    # 获取数据集的大小, 用于打印当前训练的批次\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X,y = X.to(device=device), y.to(device=device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss:{loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码是`train_loop`函数的定义, 用于在给定的数据集上训练神经网络模型, 代码的含义如下：\n",
    "1. `def train_loop(dataloader, model, loss_fn, optimizer):`：\n",
    "   1. 这是一个名为 `train_loop` 的函数定义, 它接受四个参数：\n",
    "      1. `dataloader`（数据加载器, 用于加载训练数据批次）\n",
    "      2. `model`（神经网络模型）\n",
    "      3. `loss_fn`（损失函数）\n",
    "      4. `optimizer`（优化器）. \n",
    "2. `model.train()`：\n",
    "   1. 将模型设置为训练模式. 这会启用训练模式特定的功能, 例如批次归一化和丢弃等. \n",
    "   2. 在PyTorch中, 调用 `model.train()` 方法并不是必须的, 但通常建议在训练阶段显式调用它. \n",
    "   3. 调用 `model.train()` 主要是为了启用一些训练模式下特有的功能和行为, 以确保模型在训练过程中正确执行, 并且适应于训练数据的统计特性. \n",
    "   4. 具体来说, `model.train()` 方法会做以下几件事情：\n",
    "      1. 激活训练模式：启用一些训练模式下特有的操作, 例如批次归一化层使用当前批次的统计信息来标准化输入, 丢弃层执行丢弃操作等. \n",
    "      2. 启用梯度跟踪：默认情况下, PyTorch会在训练模式下启用梯度跟踪, 即会追踪模型参数的梯度信息, 以便后续执行反向传播和参数更新操作. \n",
    "   5. 虽然调用 `model.train()` 方法不是强制性的, 但在训练过程中使用它可以确保模型处于正确的模式下, 并且可以获得预期的行为. \n",
    "   6. 另外, 即使在一些情况下, 如评估模型或进行推理时, 调用 `model.eval()` 来将模型设置为评估模式也是一个好的实践. \n",
    "3. `size = len(dataloader.dataset)`：\n",
    "   1. 计算训练数据集的总样本数, 以便在训练过程中跟踪进度. \n",
    "4. `for batch, (X, y) in enumerate(dataloader):`：\n",
    "   1. 这是一个迭代训练数据加载器的循环. \n",
    "   2. dataloader是一个可迭代对象, 加上`enumerate`之后, 就变成了一个[可索引对象](../../../Python_Learn/python语法基础/2_数组类元素_列表、元组、字典、迭代器/可索引.ipynb), 此处使用可索引`enumerate`而非可迭代`iteror`的原因在于, 我们需要获取当前处理的是第几个数据.\n",
    "      1. 在使用 `enumerate` 而不是 `iteror` 的情况下, 主要是为了获取迭代对象的索引. 虽然 `iteror` 可以用于迭代对象, 但它并不提供索引信息. \n",
    "      2. 具体来说, 使用 `enumerate` 的好处是可以**同时获取索引**和**对应的元素**, 这在某些情况下非常有用, 特别是在需要对数据进行索引处理或者跟踪处理进度时. \n",
    "         1. 比如在训练神经网络时, 经常需要使用 `enumerate` 来遍历数据集, 并获取每个批次的索引, 以便记录训练进度或者进行其他操作. \n",
    "      3. 另外, 使用 `enumerate` 还可以提高代码的可读性和简洁性, 因为它提供了一种直观的方式来同时获取索引和元素, 而不需要额外的变量来记录索引. 这使得代码更加简洁和易于理解. \n",
    "      4. 总之, 虽然 `iteror` 也可以用于迭代对象, 但如果需要获取索引信息或者希望代码更加简洁和可读, 那么使用 `enumerate` 通常是更好的选择. \n",
    "   3. 在循环中, `enumerate(dataloader)`每次返回两个数据\n",
    "      1. 第一个数据是当前读取数据处于整个可索引对象中的位置, 即下标(索引)信息\n",
    "      2. 第二个数据是一个包含输入数据和标签的元组 (X, y)，其中 X 是输入数据，y 是相应的标签. \n",
    "   4. 因此, 循环中的每一次操作, 我们都将获得一个批次的输入数据 `X` 和相应的标签 `y`, 并用`batch`获得他们当前是第几个数据\n",
    "\n",
    "5. `pred = model(X)`：\n",
    "   1. 使用模型进行前向传播, 得到模型的预测结果 `pred`. \n",
    "6. `loss = loss_fn(pred, y)`：\n",
    "   1. 使用给定的损失函数 `loss_fn`, 计算预测结果与真实标签之间的损失. \n",
    "7. `loss.backward()`：\n",
    "   1. 计算梯度：反向传播损失, 计算损失相对于模型参数的梯度. \n",
    "8. `optimizer.step()`：\n",
    "   1. 使用优化器来更新模型参数, 根据反向传播计算得到的梯度. \n",
    "9.  `optimizer.zero_grad()`：\n",
    "    1.  清零优化器中所有参数的梯度, 以便下一次迭代计算新的梯度. \n",
    "10. `if batch % 100 == 0:`：\n",
    "    1.  每隔一定的批次数（这里是每100个批次）, 执行以下操作. \n",
    "11. `loss, current = loss.item(), batch * len(X) + len(X)`：\n",
    "    1.  获取当前批次的损失值, 并计算当前处理的样本数量. \n",
    "12. `print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")`：\n",
    "    1.  打印当前批次的损失值以及当前处理的样本数量. \n",
    "    2.  在这个语句中，`{loss:>7f}` 和 `{current:>5d}` 分别是格式化字符串的一部分，它们用于对打印的变量进行格式化，以确保输出的字符串符合指定的格式。下面解释每个部分的含义：\n",
    "        1. `{loss:>7f}`:\n",
    "           - `loss` 是一个浮点数变量，表示损失值。\n",
    "           - `:` 后的 `>` 是格式化指令，表示右对齐。如果指定 `<` 则为左对齐。\n",
    "           - `7` 表示最小字段宽度为7个字符，如果输出的值不够7个字符宽度，则会在左侧填充空格以达到7个字符宽度。\n",
    "           - `f` 表示格式化为浮点数。\n",
    "           - 综合起来，`{loss:>7f}` 的含义是将损失值格式化为浮点数，保留小数点后的6位数字，并且右对齐，总宽度为7个字符。\n",
    "\n",
    "        2. `{current:>5d}`:\n",
    "           - `current` 是一个整数变量，表示当前处理的样本数量。\n",
    "           - `:` 后的 `>` 是格式化指令，表示右对齐。如果指定 `<` 则为左对齐。\n",
    "           - `5` 表示最小字段宽度为5个字符，如果输出的值不够5个字符宽度，则会在左侧填充空格以达到5个字符宽度。\n",
    "           - `d` 表示格式化为整数。\n",
    "           - 综合起来，`{current:>5d}` 的含义是将当前处理的样本数量格式化为整数，并且右对齐，总宽度为5个字符。\n",
    "      - 这些格式化指令可以用于控制输出字符串的格式，使其对齐和易读。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义test_loop函数\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X,y = X.to(device=device), y.to(device=device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码定义了一个名为 `test_loop` 的函数, 用于在给定的数据加载器上评估模型的性能. 下面逐行解释这段代码：\n",
    "\n",
    "1. `model.eval()`：\n",
    "   1. 将模型设置为评估模式. 在评估模式下, 模型会禁用一些训练模式下特有的功能, 例如批次归一化和丢弃层的操作. \n",
    "   2. 虽然不是必需的, 但是还是推荐在test阶段进行该操作, 因为它确保了模型处于正确的模式下, 能够取得更好的效果. \n",
    "2. `size = len(dataloader.dataset)` 和 `num_batches = len(dataloader)`：\n",
    "   1. 分别计算数据加载器中数据集的总大小和数据批次的数量. \n",
    "3. `test_loss, correct = 0, 0`：\n",
    "   1. 初始化测试损失和正确预测的样本数量. \n",
    "4. `with torch.no_grad():`：\n",
    "   1. 使用 `torch.no_grad()` 上下文管理器, 确保在评估模式下不会计算梯度. 这有助于减少不必要的梯度计算和内存使用, 特别是对于 `requires_grad=True` 的张量来说. \n",
    "5. `for X, y in dataloader:`：\n",
    "   1. 遍历数据加载器中的每个数据批次. 在每次迭代中, `X` 是输入数据, `y` 是相应的标签. \n",
    "6. `pred = model(X)`：\n",
    "   1. 使用模型进行前向传播, 得到预测结果 `pred`. \n",
    "7. `test_loss += loss_fn(pred, y).item()`：\n",
    "   1. 计算并累加当前数据批次的预测损失. \n",
    "   2. 这里使用了损失函数 `loss_fn` 对模型的预测结果 `pred` 和真实标签 `y` 进行计算. \n",
    "8. `correct += (pred.argmax(1) == y).type(torch.float).sum().item()`：\n",
    "   1. 计算并累加当前数据批次中模型正确预测的样本数量. \n",
    "   2. 首先, `pred.argmax(1)` 返回每个样本的预测类别, \n",
    "   3. 然后将其与真实标签 `y` 进行比较, 得到一个布尔值的张量, \n",
    "   4. 最后将布尔值的张量转换为浮点型张量, 并求和. \n",
    "9.  `test_loss /= num_batches` 和 `correct /= size`：\n",
    "    1.  计算平均测试损失和正确预测的样本比例. \n",
    "10. `print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")`：\n",
    "    1.  打印评估结果, 包括准确率和平均损失. \n",
    "    2.  使用格式化字符串 `{(100*correct):>0.1f}` 将准确率格式化为百分比, 保留一位小数；\n",
    "    3.  `{test_loss:>8f}` 将平均损失格式化为浮点数, 总宽度为8个字符. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、优化过程的创建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在, 我们将初始化「损失函数」与「优化器」, 并将这两个对象传递给`train_loop`与`test_loop`.\n",
    "\n",
    "如果有意愿, 可以随意调整`epochs`的值\n",
    "\n",
    "初始化「损失函数」与「优化器」的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化「损失函数」与「优化器」\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、训练过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一般来说, 在循环中重复调用`train_loop`与`test_loop`即可完成训练过程\n",
    "- 该例子中没有打印损失函数的值, 如果需要可以记录并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss:2.292420 [   64/60000]\n",
      "loss:2.276186 [ 6464/60000]\n",
      "loss:2.254463 [12864/60000]\n",
      "loss:2.255749 [19264/60000]\n",
      "loss:2.229204 [25664/60000]\n",
      "loss:2.199419 [32064/60000]\n",
      "loss:2.212619 [38464/60000]\n",
      "loss:2.176709 [44864/60000]\n",
      "loss:2.170320 [51264/60000]\n",
      "loss:2.141760 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 47.2%, Avg loss: 2.132148 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss:2.142294 [   64/60000]\n",
      "loss:2.128763 [ 6464/60000]\n",
      "loss:2.060271 [12864/60000]\n",
      "loss:2.089245 [19264/60000]\n",
      "loss:2.018003 [25664/60000]\n",
      "loss:1.953646 [32064/60000]\n",
      "loss:1.983658 [38464/60000]\n",
      "loss:1.901944 [44864/60000]\n",
      "loss:1.916574 [51264/60000]\n",
      "loss:1.830000 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 1.834447 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss:1.871033 [   64/60000]\n",
      "loss:1.839510 [ 6464/60000]\n",
      "loss:1.710800 [12864/60000]\n",
      "loss:1.768341 [19264/60000]\n",
      "loss:1.637314 [25664/60000]\n",
      "loss:1.595488 [32064/60000]\n",
      "loss:1.613824 [38464/60000]\n",
      "loss:1.527880 [44864/60000]\n",
      "loss:1.561435 [51264/60000]\n",
      "loss:1.439257 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.471154 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss:1.539981 [   64/60000]\n",
      "loss:1.511495 [ 6464/60000]\n",
      "loss:1.354914 [12864/60000]\n",
      "loss:1.438849 [19264/60000]\n",
      "loss:1.302158 [25664/60000]\n",
      "loss:1.307311 [32064/60000]\n",
      "loss:1.311279 [38464/60000]\n",
      "loss:1.255827 [44864/60000]\n",
      "loss:1.295438 [51264/60000]\n",
      "loss:1.183252 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 1.219754 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss:1.294022 [   64/60000]\n",
      "loss:1.286035 [ 6464/60000]\n",
      "loss:1.113794 [12864/60000]\n",
      "loss:1.230356 [19264/60000]\n",
      "loss:1.086829 [25664/60000]\n",
      "loss:1.116833 [32064/60000]\n",
      "loss:1.129981 [38464/60000]\n",
      "loss:1.088119 [44864/60000]\n",
      "loss:1.130046 [51264/60000]\n",
      "loss:1.037060 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 1.065160 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss:1.131059 [   64/60000]\n",
      "loss:1.146484 [ 6464/60000]\n",
      "loss:0.956406 [12864/60000]\n",
      "loss:1.101242 [19264/60000]\n",
      "loss:0.955814 [25664/60000]\n",
      "loss:0.989251 [32064/60000]\n",
      "loss:1.021361 [38464/60000]\n",
      "loss:0.982822 [44864/60000]\n",
      "loss:1.021512 [51264/60000]\n",
      "loss:0.947462 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.965926 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss:1.018646 [   64/60000]\n",
      "loss:1.056732 [ 6464/60000]\n",
      "loss:0.848850 [12864/60000]\n",
      "loss:1.015059 [19264/60000]\n",
      "loss:0.873154 [25664/60000]\n",
      "loss:0.899872 [32064/60000]\n",
      "loss:0.951461 [38464/60000]\n",
      "loss:0.914673 [44864/60000]\n",
      "loss:0.945358 [51264/60000]\n",
      "loss:0.887326 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.897863 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss:0.936037 [   64/60000]\n",
      "loss:0.994112 [ 6464/60000]\n",
      "loss:0.771237 [12864/60000]\n",
      "loss:0.953106 [19264/60000]\n",
      "loss:0.818049 [25664/60000]\n",
      "loss:0.834370 [32064/60000]\n",
      "loss:0.902306 [38464/60000]\n",
      "loss:0.869002 [44864/60000]\n",
      "loss:0.889636 [51264/60000]\n",
      "loss:0.843138 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.848417 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss:0.872203 [   64/60000]\n",
      "loss:0.946783 [ 6464/60000]\n",
      "loss:0.712860 [12864/60000]\n",
      "loss:0.906336 [19264/60000]\n",
      "loss:0.778659 [25664/60000]\n",
      "loss:0.784933 [32064/60000]\n",
      "loss:0.864907 [38464/60000]\n",
      "loss:0.837254 [44864/60000]\n",
      "loss:0.847542 [51264/60000]\n",
      "loss:0.808710 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.810612 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss:0.820731 [   64/60000]\n",
      "loss:0.908346 [ 6464/60000]\n",
      "loss:0.666909 [12864/60000]\n",
      "loss:0.869636 [19264/60000]\n",
      "loss:0.748732 [25664/60000]\n",
      "loss:0.746801 [32064/60000]\n",
      "loss:0.834385 [38464/60000]\n",
      "loss:0.813889 [44864/60000]\n",
      "loss:0.814797 [51264/60000]\n",
      "loss:0.780562 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.780353 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss:0.777871 [   64/60000]\n",
      "loss:0.875289 [ 6464/60000]\n",
      "loss:0.629568 [12864/60000]\n",
      "loss:0.840029 [19264/60000]\n",
      "loss:0.724902 [25664/60000]\n",
      "loss:0.716572 [32064/60000]\n",
      "loss:0.807998 [38464/60000]\n",
      "loss:0.795450 [44864/60000]\n",
      "loss:0.788277 [51264/60000]\n",
      "loss:0.756557 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.1%, Avg loss: 0.755081 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss:0.741242 [   64/60000]\n",
      "loss:0.845799 [ 6464/60000]\n",
      "loss:0.598338 [12864/60000]\n",
      "loss:0.815552 [19264/60000]\n",
      "loss:0.705092 [25664/60000]\n",
      "loss:0.692077 [32064/60000]\n",
      "loss:0.784395 [38464/60000]\n",
      "loss:0.779893 [44864/60000]\n",
      "loss:0.766087 [51264/60000]\n",
      "loss:0.735450 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.733175 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss:0.709246 [   64/60000]\n",
      "loss:0.818735 [ 6464/60000]\n",
      "loss:0.571468 [12864/60000]\n",
      "loss:0.794698 [19264/60000]\n",
      "loss:0.687969 [25664/60000]\n",
      "loss:0.671834 [32064/60000]\n",
      "loss:0.762717 [38464/60000]\n",
      "loss:0.766195 [44864/60000]\n",
      "loss:0.746995 [51264/60000]\n",
      "loss:0.716406 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.713676 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss:0.680871 [   64/60000]\n",
      "loss:0.793688 [ 6464/60000]\n",
      "loss:0.547983 [12864/60000]\n",
      "loss:0.776433 [19264/60000]\n",
      "loss:0.672946 [25664/60000]\n",
      "loss:0.654664 [32064/60000]\n",
      "loss:0.742345 [38464/60000]\n",
      "loss:0.753849 [44864/60000]\n",
      "loss:0.730322 [51264/60000]\n",
      "loss:0.699024 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.695998 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss:0.655398 [   64/60000]\n",
      "loss:0.770386 [ 6464/60000]\n",
      "loss:0.527182 [12864/60000]\n",
      "loss:0.760120 [19264/60000]\n",
      "loss:0.659687 [25664/60000]\n",
      "loss:0.639816 [32064/60000]\n",
      "loss:0.723077 [38464/60000]\n",
      "loss:0.742436 [44864/60000]\n",
      "loss:0.715479 [51264/60000]\n",
      "loss:0.682864 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.679799 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss:0.632576 [   64/60000]\n",
      "loss:0.748684 [ 6464/60000]\n",
      "loss:0.508595 [12864/60000]\n",
      "loss:0.745312 [19264/60000]\n",
      "loss:0.647851 [25664/60000]\n",
      "loss:0.626883 [32064/60000]\n",
      "loss:0.704840 [38464/60000]\n",
      "loss:0.732088 [44864/60000]\n",
      "loss:0.702221 [51264/60000]\n",
      "loss:0.667646 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 0.664893 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss:0.612044 [   64/60000]\n",
      "loss:0.728583 [ 6464/60000]\n",
      "loss:0.491869 [12864/60000]\n",
      "loss:0.731745 [19264/60000]\n",
      "loss:0.637280 [25664/60000]\n",
      "loss:0.615681 [32064/60000]\n",
      "loss:0.687582 [38464/60000]\n",
      "loss:0.722773 [44864/60000]\n",
      "loss:0.690467 [51264/60000]\n",
      "loss:0.653441 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.651176 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss:0.593413 [   64/60000]\n",
      "loss:0.709994 [ 6464/60000]\n",
      "loss:0.476883 [12864/60000]\n",
      "loss:0.719226 [19264/60000]\n",
      "loss:0.627764 [25664/60000]\n",
      "loss:0.605870 [32064/60000]\n",
      "loss:0.671353 [38464/60000]\n",
      "loss:0.714599 [44864/60000]\n",
      "loss:0.680244 [51264/60000]\n",
      "loss:0.640088 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.8%, Avg loss: 0.638559 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss:0.576502 [   64/60000]\n",
      "loss:0.692797 [ 6464/60000]\n",
      "loss:0.463335 [12864/60000]\n",
      "loss:0.707663 [19264/60000]\n",
      "loss:0.619203 [25664/60000]\n",
      "loss:0.597158 [32064/60000]\n",
      "loss:0.656139 [38464/60000]\n",
      "loss:0.707550 [44864/60000]\n",
      "loss:0.671323 [51264/60000]\n",
      "loss:0.627512 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.626948 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss:0.561103 [   64/60000]\n",
      "loss:0.676952 [ 6464/60000]\n",
      "loss:0.451033 [12864/60000]\n",
      "loss:0.696781 [19264/60000]\n",
      "loss:0.611461 [25664/60000]\n",
      "loss:0.589311 [32064/60000]\n",
      "loss:0.641931 [38464/60000]\n",
      "loss:0.701505 [44864/60000]\n",
      "loss:0.663612 [51264/60000]\n",
      "loss:0.615676 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.616275 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss:0.547030 [   64/60000]\n",
      "loss:0.662308 [ 6464/60000]\n",
      "loss:0.439828 [12864/60000]\n",
      "loss:0.686564 [19264/60000]\n",
      "loss:0.604436 [25664/60000]\n",
      "loss:0.582215 [32064/60000]\n",
      "loss:0.628664 [38464/60000]\n",
      "loss:0.696426 [44864/60000]\n",
      "loss:0.657005 [51264/60000]\n",
      "loss:0.604431 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.606449 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss:0.534093 [   64/60000]\n",
      "loss:0.648798 [ 6464/60000]\n",
      "loss:0.429595 [12864/60000]\n",
      "loss:0.677006 [19264/60000]\n",
      "loss:0.597943 [25664/60000]\n",
      "loss:0.575689 [32064/60000]\n",
      "loss:0.616398 [38464/60000]\n",
      "loss:0.692300 [44864/60000]\n",
      "loss:0.651328 [51264/60000]\n",
      "loss:0.593790 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.597402 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss:0.522103 [   64/60000]\n",
      "loss:0.636383 [ 6464/60000]\n",
      "loss:0.420244 [12864/60000]\n",
      "loss:0.668054 [19264/60000]\n",
      "loss:0.591835 [25664/60000]\n",
      "loss:0.569663 [32064/60000]\n",
      "loss:0.604960 [38464/60000]\n",
      "loss:0.688968 [44864/60000]\n",
      "loss:0.646550 [51264/60000]\n",
      "loss:0.583597 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: 0.589064 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss:0.510933 [   64/60000]\n",
      "loss:0.624925 [ 6464/60000]\n",
      "loss:0.411656 [12864/60000]\n",
      "loss:0.659611 [19264/60000]\n",
      "loss:0.585972 [25664/60000]\n",
      "loss:0.563959 [32064/60000]\n",
      "loss:0.594283 [38464/60000]\n",
      "loss:0.686425 [44864/60000]\n",
      "loss:0.642403 [51264/60000]\n",
      "loss:0.573840 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.581364 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss:0.500455 [   64/60000]\n",
      "loss:0.614379 [ 6464/60000]\n",
      "loss:0.403787 [12864/60000]\n",
      "loss:0.651622 [19264/60000]\n",
      "loss:0.580216 [25664/60000]\n",
      "loss:0.558493 [32064/60000]\n",
      "loss:0.584392 [38464/60000]\n",
      "loss:0.684520 [44864/60000]\n",
      "loss:0.638844 [51264/60000]\n",
      "loss:0.564531 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.574237 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss:0.490672 [   64/60000]\n",
      "loss:0.604692 [ 6464/60000]\n",
      "loss:0.396555 [12864/60000]\n",
      "loss:0.644088 [19264/60000]\n",
      "loss:0.574623 [25664/60000]\n",
      "loss:0.553246 [32064/60000]\n",
      "loss:0.575194 [38464/60000]\n",
      "loss:0.683177 [44864/60000]\n",
      "loss:0.635771 [51264/60000]\n",
      "loss:0.555569 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.567635 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss:0.481568 [   64/60000]\n",
      "loss:0.595749 [ 6464/60000]\n",
      "loss:0.389786 [12864/60000]\n",
      "loss:0.636887 [19264/60000]\n",
      "loss:0.569104 [25664/60000]\n",
      "loss:0.548292 [32064/60000]\n",
      "loss:0.566696 [38464/60000]\n",
      "loss:0.682374 [44864/60000]\n",
      "loss:0.633159 [51264/60000]\n",
      "loss:0.546923 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.561517 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss:0.472975 [   64/60000]\n",
      "loss:0.587500 [ 6464/60000]\n",
      "loss:0.383419 [12864/60000]\n",
      "loss:0.630036 [19264/60000]\n",
      "loss:0.563676 [25664/60000]\n",
      "loss:0.543424 [32064/60000]\n",
      "loss:0.558757 [38464/60000]\n",
      "loss:0.681972 [44864/60000]\n",
      "loss:0.630832 [51264/60000]\n",
      "loss:0.538544 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.555834 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss:0.464848 [   64/60000]\n",
      "loss:0.579875 [ 6464/60000]\n",
      "loss:0.377506 [12864/60000]\n",
      "loss:0.623568 [19264/60000]\n",
      "loss:0.558273 [25664/60000]\n",
      "loss:0.538665 [32064/60000]\n",
      "loss:0.551380 [38464/60000]\n",
      "loss:0.681856 [44864/60000]\n",
      "loss:0.628761 [51264/60000]\n",
      "loss:0.530441 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.550535 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss:0.457180 [   64/60000]\n",
      "loss:0.572859 [ 6464/60000]\n",
      "loss:0.371994 [12864/60000]\n",
      "loss:0.617412 [19264/60000]\n",
      "loss:0.552955 [25664/60000]\n",
      "loss:0.533966 [32064/60000]\n",
      "loss:0.544505 [38464/60000]\n",
      "loss:0.681995 [44864/60000]\n",
      "loss:0.626864 [51264/60000]\n",
      "loss:0.522726 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.545598 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss:0.449928 [   64/60000]\n",
      "loss:0.566402 [ 6464/60000]\n",
      "loss:0.366808 [12864/60000]\n",
      "loss:0.611556 [19264/60000]\n",
      "loss:0.547680 [25664/60000]\n",
      "loss:0.529318 [32064/60000]\n",
      "loss:0.538109 [38464/60000]\n",
      "loss:0.682361 [44864/60000]\n",
      "loss:0.625103 [51264/60000]\n",
      "loss:0.515337 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.540988 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss:0.442990 [   64/60000]\n",
      "loss:0.560437 [ 6464/60000]\n",
      "loss:0.361942 [12864/60000]\n",
      "loss:0.605930 [19264/60000]\n",
      "loss:0.542479 [25664/60000]\n",
      "loss:0.524736 [32064/60000]\n",
      "loss:0.532177 [38464/60000]\n",
      "loss:0.682855 [44864/60000]\n",
      "loss:0.623444 [51264/60000]\n",
      "loss:0.508210 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.536674 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss:0.436402 [   64/60000]\n",
      "loss:0.554898 [ 6464/60000]\n",
      "loss:0.357438 [12864/60000]\n",
      "loss:0.600548 [19264/60000]\n",
      "loss:0.537354 [25664/60000]\n",
      "loss:0.520180 [32064/60000]\n",
      "loss:0.526684 [38464/60000]\n",
      "loss:0.683462 [44864/60000]\n",
      "loss:0.621814 [51264/60000]\n",
      "loss:0.501352 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.532634 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss:0.430076 [   64/60000]\n",
      "loss:0.549701 [ 6464/60000]\n",
      "loss:0.353198 [12864/60000]\n",
      "loss:0.595439 [19264/60000]\n",
      "loss:0.532329 [25664/60000]\n",
      "loss:0.515743 [32064/60000]\n",
      "loss:0.521507 [38464/60000]\n",
      "loss:0.684078 [44864/60000]\n",
      "loss:0.620262 [51264/60000]\n",
      "loss:0.494830 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.528845 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss:0.424045 [   64/60000]\n",
      "loss:0.544888 [ 6464/60000]\n",
      "loss:0.349247 [12864/60000]\n",
      "loss:0.590486 [19264/60000]\n",
      "loss:0.527375 [25664/60000]\n",
      "loss:0.511410 [32064/60000]\n",
      "loss:0.516635 [38464/60000]\n",
      "loss:0.684704 [44864/60000]\n",
      "loss:0.618729 [51264/60000]\n",
      "loss:0.488580 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.525283 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss:0.418260 [   64/60000]\n",
      "loss:0.540406 [ 6464/60000]\n",
      "loss:0.345510 [12864/60000]\n",
      "loss:0.585724 [19264/60000]\n",
      "loss:0.522562 [25664/60000]\n",
      "loss:0.507187 [32064/60000]\n",
      "loss:0.512035 [38464/60000]\n",
      "loss:0.685264 [44864/60000]\n",
      "loss:0.617204 [51264/60000]\n",
      "loss:0.482592 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.521923 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss:0.412735 [   64/60000]\n",
      "loss:0.536220 [ 6464/60000]\n",
      "loss:0.341986 [12864/60000]\n",
      "loss:0.581124 [19264/60000]\n",
      "loss:0.517847 [25664/60000]\n",
      "loss:0.503044 [32064/60000]\n",
      "loss:0.507703 [38464/60000]\n",
      "loss:0.685711 [44864/60000]\n",
      "loss:0.615705 [51264/60000]\n",
      "loss:0.476908 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.518746 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss:0.407430 [   64/60000]\n",
      "loss:0.532295 [ 6464/60000]\n",
      "loss:0.338661 [12864/60000]\n",
      "loss:0.576677 [19264/60000]\n",
      "loss:0.513202 [25664/60000]\n",
      "loss:0.498964 [32064/60000]\n",
      "loss:0.503659 [38464/60000]\n",
      "loss:0.685991 [44864/60000]\n",
      "loss:0.614103 [51264/60000]\n",
      "loss:0.471483 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.515739 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss:0.402296 [   64/60000]\n",
      "loss:0.528641 [ 6464/60000]\n",
      "loss:0.335530 [12864/60000]\n",
      "loss:0.572402 [19264/60000]\n",
      "loss:0.508720 [25664/60000]\n",
      "loss:0.494988 [32064/60000]\n",
      "loss:0.499815 [38464/60000]\n",
      "loss:0.686083 [44864/60000]\n",
      "loss:0.612527 [51264/60000]\n",
      "loss:0.466368 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.512888 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss:0.397350 [   64/60000]\n",
      "loss:0.525218 [ 6464/60000]\n",
      "loss:0.332494 [12864/60000]\n",
      "loss:0.568290 [19264/60000]\n",
      "loss:0.504306 [25664/60000]\n",
      "loss:0.491203 [32064/60000]\n",
      "loss:0.496174 [38464/60000]\n",
      "loss:0.685969 [44864/60000]\n",
      "loss:0.610936 [51264/60000]\n",
      "loss:0.461518 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.510179 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss:0.392597 [   64/60000]\n",
      "loss:0.522006 [ 6464/60000]\n",
      "loss:0.329593 [12864/60000]\n",
      "loss:0.564372 [19264/60000]\n",
      "loss:0.500002 [25664/60000]\n",
      "loss:0.487570 [32064/60000]\n",
      "loss:0.492723 [38464/60000]\n",
      "loss:0.685728 [44864/60000]\n",
      "loss:0.609348 [51264/60000]\n",
      "loss:0.456936 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.507600 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss:0.388023 [   64/60000]\n",
      "loss:0.518968 [ 6464/60000]\n",
      "loss:0.326881 [12864/60000]\n",
      "loss:0.560583 [19264/60000]\n",
      "loss:0.495805 [25664/60000]\n",
      "loss:0.484051 [32064/60000]\n",
      "loss:0.489453 [38464/60000]\n",
      "loss:0.685390 [44864/60000]\n",
      "loss:0.607730 [51264/60000]\n",
      "loss:0.452561 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.505139 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss:0.383650 [   64/60000]\n",
      "loss:0.516084 [ 6464/60000]\n",
      "loss:0.324313 [12864/60000]\n",
      "loss:0.556974 [19264/60000]\n",
      "loss:0.491738 [25664/60000]\n",
      "loss:0.480673 [32064/60000]\n",
      "loss:0.486317 [38464/60000]\n",
      "loss:0.684901 [44864/60000]\n",
      "loss:0.606123 [51264/60000]\n",
      "loss:0.448470 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.502792 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss:0.379395 [   64/60000]\n",
      "loss:0.513386 [ 6464/60000]\n",
      "loss:0.321852 [12864/60000]\n",
      "loss:0.553514 [19264/60000]\n",
      "loss:0.487840 [25664/60000]\n",
      "loss:0.477420 [32064/60000]\n",
      "loss:0.483298 [38464/60000]\n",
      "loss:0.684272 [44864/60000]\n",
      "loss:0.604552 [51264/60000]\n",
      "loss:0.444616 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.500550 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss:0.375335 [   64/60000]\n",
      "loss:0.510815 [ 6464/60000]\n",
      "loss:0.319440 [12864/60000]\n",
      "loss:0.550177 [19264/60000]\n",
      "loss:0.484043 [25664/60000]\n",
      "loss:0.474270 [32064/60000]\n",
      "loss:0.480351 [38464/60000]\n",
      "loss:0.683424 [44864/60000]\n",
      "loss:0.602975 [51264/60000]\n",
      "loss:0.440967 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.498403 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss:0.371394 [   64/60000]\n",
      "loss:0.508373 [ 6464/60000]\n",
      "loss:0.317170 [12864/60000]\n",
      "loss:0.546940 [19264/60000]\n",
      "loss:0.480380 [25664/60000]\n",
      "loss:0.471246 [32064/60000]\n",
      "loss:0.477530 [38464/60000]\n",
      "loss:0.682392 [44864/60000]\n",
      "loss:0.601368 [51264/60000]\n",
      "loss:0.437490 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.496342 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss:0.367593 [   64/60000]\n",
      "loss:0.506043 [ 6464/60000]\n",
      "loss:0.314958 [12864/60000]\n",
      "loss:0.543867 [19264/60000]\n",
      "loss:0.476804 [25664/60000]\n",
      "loss:0.468292 [32064/60000]\n",
      "loss:0.474805 [38464/60000]\n",
      "loss:0.681266 [44864/60000]\n",
      "loss:0.599744 [51264/60000]\n",
      "loss:0.434191 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.494360 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss:0.363843 [   64/60000]\n",
      "loss:0.503797 [ 6464/60000]\n",
      "loss:0.312835 [12864/60000]\n",
      "loss:0.540896 [19264/60000]\n",
      "loss:0.473366 [25664/60000]\n",
      "loss:0.465447 [32064/60000]\n",
      "loss:0.472191 [38464/60000]\n",
      "loss:0.680104 [44864/60000]\n",
      "loss:0.598081 [51264/60000]\n",
      "loss:0.431100 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.492450 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss:0.360221 [   64/60000]\n",
      "loss:0.501654 [ 6464/60000]\n",
      "loss:0.310776 [12864/60000]\n",
      "loss:0.538032 [19264/60000]\n",
      "loss:0.469968 [25664/60000]\n",
      "loss:0.462686 [32064/60000]\n",
      "loss:0.469652 [38464/60000]\n",
      "loss:0.678831 [44864/60000]\n",
      "loss:0.596415 [51264/60000]\n",
      "loss:0.428177 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.490604 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss:0.356707 [   64/60000]\n",
      "loss:0.499583 [ 6464/60000]\n",
      "loss:0.308766 [12864/60000]\n",
      "loss:0.535274 [19264/60000]\n",
      "loss:0.466619 [25664/60000]\n",
      "loss:0.459996 [32064/60000]\n",
      "loss:0.467202 [38464/60000]\n",
      "loss:0.677484 [44864/60000]\n",
      "loss:0.594746 [51264/60000]\n",
      "loss:0.425410 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.488820 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    # 每个epch中, 应先有一个训练过程\n",
    "    train_loop(\n",
    "        dataloader=train_dataloader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "\n",
    "    # 每个epch中, 在train后, 应该有一个test过程\n",
    "    test_loop(\n",
    "        dataloader=test_dataloader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn\n",
    "    )\n",
    "\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、网络结构与参数的保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练完毕后, 我们可以将网络结构与训练获得的参数保存下来, 防止训练数据的丢失, 工作流程如下：\n",
    "1. 保存网络结构\n",
    "2. 保存网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络结构保存\n",
    "torch.save(model, './data/model_and_weights_for_z/model.pth')\n",
    "\n",
    "# 网络状态与参数保存\n",
    "torch.save(model.state_dict(), './data/model_and_weights_for_z/model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、网络结构与参数的加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果保存了网络结构与参数, 则可以在其他程序中使用该网络, 加载过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载网络\n",
    "model = torch.load('./data/model_and_weights_for_z/model.pth')\n",
    "\n",
    "# 加载网络参数\n",
    "model_state_dict = torch.load('./data/model_and_weights_for_z/model_state_dict.pth')\n",
    "\n",
    "# 将网络参数加载入网络中\n",
    "model.load_state_dict(state_dict=model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七、网络的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 从数据集中取标签与数据\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "\n",
    "# 使用no_grad上下文管理器\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    \n",
    "    # 直接使用model(图像)即可调用网络\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
